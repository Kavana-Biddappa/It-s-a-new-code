import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler
import sqlite3  
import datetime

# Step 1: Simulated Dummy Dataset
np.random.seed(42)
num_samples = 100000

data = {
    'result': np.random.randint(700, 1001, num_samples),  # Target variable
    'day': np.random.randint(1, 32, num_samples),
    'week': np.random.randint(1, 53, num_samples),
    'month': np.random.randint(1, 13, num_samples),
    'year': np.random.randint(2020, 2024, num_samples),
}

df = pd.DataFrame(data)

# Step 2: Data Preprocessing
scaler = StandardScaler()
features = df[['day', 'week', 'month', 'year']]
target = df['result'].values.reshape(-1, 1)

features_scaled = scaler.fit_transform(features)
X = features_scaled.reshape((features_scaled.shape[0], 1, features_scaled.shape[1]))
y = target

# Step 3: Define LSTM Model
from tensorflow.keras.regularizers import l2

def build_lstm_model():
    model = Sequential()
    model.add(LSTM(units=50, return_sequences=True, input_shape=(1, 4)))  
    model.add(Dropout(0.4))
    model.add(LSTM(units=50))
    model.add(Dense(25, activation='relu', kernel_regularizer=l2(0.001)))
    model.add(Dropout(0.4))
    model.add(Dense(1, activation='linear'))

    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
    return model

model = build_lstm_model()

# Step 4: Initial Model Training
model.fit(X, y, epochs=10, batch_size=1024, verbose=1)

# Step 5: Log Last Used Time
last_used_time = datetime.datetime.now()
print("Model last used at:", last_used_time)

# Step 6: Fetch New Data from Database
def fetch_new_data(last_used_time):
    conn = sqlite3.connect('cloud_database.db')  
    query = f"""
        SELECT result, day, week, month, year FROM data_table
        WHERE timestamp > '{last_used_time.strftime('%Y-%m-%d %H:%M:%S')}'
    """
    new_data = pd.read_sql_query(query, conn)
    conn.close()
    return new_data

# Simulating new data arrival
new_data = fetch_new_data(last_used_time)

if not new_data.empty:
    print(f"Fetched {len(new_data)} new records for incremental training")

# Step 7: Incremental Training with Anti-Forgetting Techniques
def incremental_train(new_data, model, scaler):
    if new_data.empty:
        print("No new data. Skipping training.")
        return model

    new_features = new_data[['day', 'week', 'month', 'year']]
    new_features_scaled = scaler.transform(new_features)
    X_new = new_features_scaled.reshape((new_features_scaled.shape[0], 1, new_features_scaled.shape[1]))
    y_new = new_data['result'].values.reshape(-1, 1)

    old_data_sample = df.sample(frac=0.1, random_state=42)
    old_features_scaled = scaler.transform(old_data_sample[['day', 'week', 'month', 'year']])
    X_old = old_features_scaled.reshape((old_features_scaled.shape[0], 1, old_features_scaled.shape[1]))
    y_old = old_data_sample['result'].values.reshape(-1, 1)

    X_combined = np.concatenate((X_new, X_old), axis=0)
    y_combined = np.concatenate((y_new, y_old), axis=0)

    model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')

    for batch in range(0, len(X_combined), 256):
        X_batch = X_combined[batch: batch+256]
        y_batch = y_combined[batch: batch+256]
        model.fit(X_batch, y_batch, epochs=1, batch_size=256, verbose=1)

    return model

model = incremental_train(new_data, model, scaler)

# Step 8: Predicting Future Values
def predict_future(model, future_dates):
    future_df = pd.DataFrame(future_dates, columns=['day', 'week', 'month', 'year'])
    future_scaled = scaler.transform(future_df)
    X_future = future_scaled.reshape((future_scaled.shape[0], 1, future_scaled.shape[1]))
    predictions = model.predict(X_future)
    return predictions

# Example: Predict for next 10 days
future_dates = [
    [5, 10, 2, 2024], [6, 10, 2, 2024], [7, 10, 2, 2024],  
]

predictions = predict_future(model, future_dates)
print("Future Predictions:", predictions)

# Step 9: Update Last Used Timestamp
last_used_time = datetime.datetime.now()
print("Model last used at:", last_used_time)