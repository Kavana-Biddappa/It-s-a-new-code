import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import joblib
from datetime import datetime

# Load and clean your data
data = pd.read_csv('your_dataset.csv')  # Replace with your actual file path

# Parse date and time column, convert it to datetime
data['Date_Time'] = pd.to_datetime(data['Date_Time'], format='%Y-%m-%d %H:%M:%S')

# Handle mixed-type columns with OneHotEncoding for categorical columns
categorical_cols = ['Family', 'Downtime_Description', 'Downtime_Category', 'Part_Number']  # Adjust based on your actual columns
encoders = {}
for col in categorical_cols:
    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
    encoded = encoder.fit_transform(data[[col]].astype(str))  # Ensure data is treated as strings
    encoders[col] = encoder
    encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out([col]))
    data = pd.concat([data.drop(columns=[col]), encoded_df], axis=1)

# Scale numerical columns
numerical_cols = ['Cycle_Time', 'Target_Parts', 'Parts_Produced', 'Downtime']  # Specify actual numerical columns
scaler = MinMaxScaler()
scaled_numerical_data = scaler.fit_transform(data[numerical_cols])
scaled_data = np.hstack([scaled_numerical_data, data.drop(columns=numerical_cols + ['Date_Time']).values])

# Define features (X) and labels (y)
X = scaled_data[:, :-9]  # Last 9 columns as targets; adjust based on exact structure
y = scaled_data[:, -9:]

# Reshape X for LSTM input
X = X.reshape((X.shape[0], 1, X.shape[1]))

# Split the data for initial training
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Build the LSTM model
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),
    Dropout(0.2),
    LSTM(64),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(9)  # Adjust if more than 9 output targets
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model with early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stop], verbose=1)

# Save model and scaler
model.save('trained_lstm_model.h5')
joblib.dump(scaler, 'scaler.save')

# Function to predict next hour based on the latest data row
def predict_next_hour(data, model, scaler, encoders):
    # Get the last row and prepare it for prediction
    last_row = data.iloc[-1].drop('Date_Time').values.reshape(1, -1)
    
    # Scale last row data
    scaled_input = scaler.transform(last_row)
    scaled_input = scaled_input.reshape((1, 1, scaled_input.shape[1]))

    # Predict the next hourâ€™s values
    prediction = model.predict(scaled_input)
    
    # Inverse scale the prediction
    full_prediction = np.hstack([scaled_input[0, 0, :-9], prediction])
    prediction_unscaled = scaler.inverse_transform(full_prediction.reshape(1, -1))[:, -9:]
    
    return prediction_unscaled[0]

# Example usage for predicting the next hour
predicted_next_hour = predict_next_hour(data, model, scaler, encoders)
print("Predicted values for the next hour:", predicted_next_hour)

# Continual training function to update the model with new data
def update_model_with_live_data(new_data, model, scaler, encoders):
    # Add new data row to the dataset
    data = data.append(new_data, ignore_index=True)
    
    # Preprocess the new data row (handle encoding and scaling)
    for col, encoder in encoders.items():
        encoded_col = encoder.transform([[str(new_data[col])]])
        new_data = new_data.drop(columns=[col]).join(pd.DataFrame(encoded_col, columns=encoder.get_feature_names_out([col])))
    new_data_scaled = scaler.transform(new_data.values.reshape(1, -1))
    new_data_scaled = new_data_scaled.reshape((1, 1, new_data_scaled.shape[1]))

    # Fit model to new data
    model.fit(new_data_scaled, new_data_scaled[:, :, -9:], epochs=1, batch_size=1, verbose=0)

    # Save the updated model
    model.save('updated_lstm_model.h5')
    return model

# Sample new live data (replace with actual incoming data)
new_data_row = {
    'Date_Time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'Family': 'ExampleFamily',
    'Downtime_Description': 'ExampleCause',
    'Downtime_Category': 'ExampleCategory',
    'Part_Number': '123ABC',
    'Cycle_Time': 30.0,
    'Target_Parts': 200,
    'Parts_Produced': 195,
    'Downtime': 5
}

# Process new data row and update model
model = update_model_with_live_data(pd.DataFrame([new_data_row]), model, scaler, encoders)